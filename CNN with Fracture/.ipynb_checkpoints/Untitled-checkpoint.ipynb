{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " == MNIST ==\n",
      "X_train shape: (80L, 1L, 144L, 144L)\n",
      "80 train samples\n",
      "20 test samples\n",
      "   - New residual block with\n",
      "('      input shape:', (128, 148, 148))\n",
      "('      kernel size:', [3, 3])\n",
      "        -- model was built.\n",
      "   - New residual block with\n",
      "('      input shape:', (128, 150, 150))\n",
      "('      kernel size:', [3, 3])\n",
      "      - Input channels: 128 ---> num feature maps on out: 256\n",
      "('      - with subsample:', (2, 2))\n",
      "        -- model was built.\n",
      "   - New residual block with\n",
      "('      input shape:', (256, 74, 74))\n",
      "('      kernel size:', [3, 3])\n",
      "        -- model was built.\n",
      "   - New residual block with\n",
      "('      input shape:', (256, 76, 76))\n",
      "('      kernel size:', [3, 3])\n",
      "      - Input channels: 256 ---> num feature maps on out: 512\n",
      "('      - with subsample:', (2, 2))\n",
      "        -- model was built.\n",
      "   - New residual block with\n",
      "('      input shape:', (512, 37, 37))\n",
      "('      kernel size:', [3, 3])\n",
      "        -- model was built.\n",
      "   - New residual block with\n",
      "('      input shape:', (512, 37, 37))\n",
      "('      kernel size:', [3, 3])\n",
      "('      - with subsample:', (2, 2))\n",
      "        -- model was built.\n",
      "Average pooling, from (18,18) to (1,1)\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "80/80 [==============================] - 140s - loss: 0.4015 - acc: 0.0125 - val_loss: 0.4146 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "80/80 [==============================] - 139s - loss: 0.4015 - acc: 0.0125 - val_loss: 0.4146 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "80/80 [==============================] - 137s - loss: 0.4015 - acc: 0.0125 - val_loss: 0.4146 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "80/80 [==============================] - 138s - loss: 0.4015 - acc: 0.0125 - val_loss: 0.4146 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "80/80 [==============================] - 138s - loss: 0.4015 - acc: 0.0125 - val_loss: 0.4146 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "80/80 [==============================] - 137s - loss: 0.4015 - acc: 0.0125 - val_loss: 0.4146 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "80/80 [==============================] - 137s - loss: 0.4015 - acc: 0.0125 - val_loss: 0.4146 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "14/80 [====>.........................] - ETA: 107s - loss: 0.5124 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6d062a6f392f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n\u001b[1;32m--> 168\u001b[1;33m               verbose=1, validation_data=(X_test, Y_test))#, callbacks=[best_model])\n\u001b[0m\u001b[0;32m    169\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;31m#     print('Test score:', score[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\lib\\site-packages\\keras\\models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m                               sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mc:\\anaconda\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1102\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[0;32m   1105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[0;32m    820\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\lib\\site-packages\\keras\\backend\\theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    715\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "sys.setrecursionlimit(99999)\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist, cifar10\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import ZeroPadding2D, AveragePooling2D, Convolution2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "import residual_blocks\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "nb_classes = 1\n",
    "nb_epoch = 20\n",
    "\n",
    "def compute_padding_length(length_before, stride, length_conv):\n",
    "    ''' Assumption: you want the subsampled result has a length of floor(original_length/stride).\n",
    "    '''\n",
    "    N = length_before\n",
    "    F = length_conv\n",
    "    S = stride\n",
    "    if S == F:\n",
    "        return 0\n",
    "    if S == 1:\n",
    "        return (F-1)/2\n",
    "    for P in range(S):\n",
    "        if (N-F+2*P)/S + 1 == N/S:\n",
    "            return P\n",
    "    return None\n",
    "\n",
    "def design_for_residual_blocks(num_channel_input=1):\n",
    "    ''''''\n",
    "    model = Sequential() # it's a CONTAINER, not MODEL\n",
    "    # set numbers\n",
    "    num_big_blocks = 3\n",
    "    image_patch_sizes = [[3,3]]*num_big_blocks\n",
    "    pool_sizes = [(2,2)]*num_big_blocks\n",
    "    n_features = [128, 256, 512, 512, 1024]\n",
    "    n_features_next = [256, 512, 512, 512, 1024]\n",
    "    height_input = 148\n",
    "    width_input = 148\n",
    "    for conv_idx in range(num_big_blocks):    \n",
    "        n_feat_here = n_features[conv_idx]\n",
    "        # residual block 0\n",
    "        model.add(residual_blocks.building_residual_block(  (num_channel_input, height_input, width_input),\n",
    "                                                            n_feat_here,\n",
    "                                                            kernel_sizes=image_patch_sizes[conv_idx]\n",
    "                                                            ))\n",
    "\n",
    "        # residual block 1 (you can add it as you want (and your resources allow..))\n",
    "        if False:\n",
    "            model.add(residual_blocks.building_residual_block(  (n_feat_here, height_input, width_input),\n",
    "                                                                n_feat_here,\n",
    "                                                                kernel_sizes=image_patch_sizes[conv_idx]\n",
    "                                                                ))\n",
    "        \n",
    "        # the last residual block N-1\n",
    "        # the last one : pad zeros, subsamples, and increase #channels\n",
    "        pad_height = compute_padding_length(height_input, pool_sizes[conv_idx][0], image_patch_sizes[conv_idx][0])\n",
    "        pad_width = compute_padding_length(width_input, pool_sizes[conv_idx][1], image_patch_sizes[conv_idx][1])\n",
    "        model.add(ZeroPadding2D(padding=(pad_height,pad_width))) \n",
    "        height_input += 2*pad_height\n",
    "        width_input += 2*pad_width\n",
    "        n_feat_next = n_features_next[conv_idx]\n",
    "        model.add(residual_blocks.building_residual_block(  (n_feat_here, height_input, width_input),\n",
    "                                                            n_feat_next,\n",
    "                                                            kernel_sizes=image_patch_sizes[conv_idx],\n",
    "                                                            is_subsample=True,\n",
    "                                                            subsample=pool_sizes[conv_idx]\n",
    "                                                            ))\n",
    "\n",
    "        height_input, width_input = model.output_shape[2:]\n",
    "        # width_input  = int(width_input/pool_sizes[conv_idx][1])\n",
    "        num_channel_input = n_feat_next\n",
    "\n",
    "    # Add average pooling at the end:\n",
    "    print('Average pooling, from (%d,%d) to (1,1)' % (height_input, width_input))\n",
    "    model.add(AveragePooling2D(pool_size=(height_input, width_input)))\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_residual_model(is_mnist=True, img_channels=1, img_rows=28, img_cols=28):\n",
    "    model = keras.models.Sequential()\n",
    "    first_layer_channel = 128\n",
    "    if is_mnist: # size to be changed to 32,32\n",
    "        model.add(ZeroPadding2D((2,2), input_shape=(img_channels, img_rows, img_cols))) # resize (28,28)-->(32,32)\n",
    "        # the first conv \n",
    "        model.add(Convolution2D(first_layer_channel, 3, 3, border_mode='same'))\n",
    "    else:\n",
    "        model.add(Convolution2D(first_layer_channel, 3, 3, border_mode='same', input_shape=(img_channels, img_rows, img_cols)))\n",
    "\n",
    "    model.add(Activation('relu'))\n",
    "    # [residual-based Conv layers]\n",
    "    residual_blocks = design_for_residual_blocks(num_channel_input=first_layer_channel)\n",
    "    model.add(residual_blocks)\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "    # [Classifier]    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('linear'))\n",
    "    # [END]\n",
    "    return model\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    \n",
    "    is_mnist = True\n",
    "    is_cifar10 = not is_mnist\n",
    "    \n",
    "    if is_mnist:\n",
    "        import scipy.io as sio\n",
    "        import numpy as np\n",
    "        img_rows, img_cols = 144, 144\n",
    "        img_channels = 1\n",
    "        WB=sio.loadmat('WB_small.mat')['WB_small']\n",
    "        Y_data=sio.loadmat('true.mat')['skt']\n",
    "        Y_data=(Y_data-min(Y_data))/(max(Y_data)-min(Y_data))\n",
    "        X_data=np.reshape(WB,(100,1,img_rows,img_cols))\n",
    "        X_train=X_data[0:80];X_test=X_data[80:100]\n",
    "        Y_train=Y_data[0:80];Y_test=Y_data[80:100]\n",
    "        print(' == MNIST ==')\n",
    "#         (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#         img_rows, img_cols = 28, 28\n",
    "#         img_channels = 1\n",
    "#         print(' == MNIST ==')\n",
    "    else:\n",
    "        (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "        img_rows, img_cols = 32, 32\n",
    "        img_channels = 3\n",
    "        print(' == CIFAR10 ==')\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_channels, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_channels, img_rows, img_cols)\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    # X_train /= 255\n",
    "    # X_test /= 255\n",
    "    #X_train = (X_train - np.mean(X_train))/np.std(X_train)\n",
    "    #X_test = (X_test - np.mean(X_test))/np.std(X_test)\n",
    "    \n",
    "    X_train = (X_train - np.mean(X_train))/np.std(X_train)\n",
    "    X_test = (X_test - np.mean(X_test))/np.std(X_test)\n",
    "#     Y_train = y_train\n",
    "#     Y_test = y_test\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print(X_train.shape[0], 'train samples')\n",
    "    print(X_test.shape[0], 'test samples')\n",
    "\n",
    "    model = get_residual_model(is_mnist=is_mnist, img_channels=img_channels, img_rows=img_rows, img_cols=img_cols)\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#     # autosave best Model\n",
    "#     best_model_file = \"./my_model_weights.h5\"\n",
    "#     best_model = ModelCheckpoint(best_model_file, verbose=1, save_best_only=True)\n",
    "    \n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "              verbose=1, validation_data=(X_test, Y_test))#, callbacks=[best_model])\n",
    "    score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n",
    "#     print('Test score:', score[0])\n",
    "#     print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43034283],\n",
       "       [ 0.41932673],\n",
       "       [ 0.47858875],\n",
       "       [ 0.49762297],\n",
       "       [ 0.51985312],\n",
       "       [ 0.55207077],\n",
       "       [ 0.55633978],\n",
       "       [ 0.20823536],\n",
       "       [ 0.41072062],\n",
       "       [ 0.36715662],\n",
       "       [ 0.1839321 ],\n",
       "       [ 0.22237378],\n",
       "       [ 0.42633667],\n",
       "       [ 0.52964757],\n",
       "       [ 0.30035597],\n",
       "       [ 0.5418037 ],\n",
       "       [ 0.26847866],\n",
       "       [ 0.4286603 ],\n",
       "       [ 0.4943752 ],\n",
       "       [ 0.27779997],\n",
       "       [ 0.28668339],\n",
       "       [ 0.30145042],\n",
       "       [ 0.17222868],\n",
       "       [ 0.49486973],\n",
       "       [ 0.53068474],\n",
       "       [ 0.64168888],\n",
       "       [ 0.5684324 ],\n",
       "       [ 0.49439536],\n",
       "       [ 0.30679581],\n",
       "       [ 0.41966861],\n",
       "       [ 0.45036067],\n",
       "       [ 0.42720107],\n",
       "       [ 0.22979805],\n",
       "       [ 0.35672436],\n",
       "       [ 0.4587432 ],\n",
       "       [ 0.46012451],\n",
       "       [ 0.2939915 ],\n",
       "       [ 0.4393664 ],\n",
       "       [ 0.50183133],\n",
       "       [ 0.33535696],\n",
       "       [ 0.41452245],\n",
       "       [ 0.44688681],\n",
       "       [ 0.25294417],\n",
       "       [ 0.38883138],\n",
       "       [ 0.44517342],\n",
       "       [ 0.38803201],\n",
       "       [ 0.65730797],\n",
       "       [ 0.38603953],\n",
       "       [ 0.23458924],\n",
       "       [ 0.70208644],\n",
       "       [ 0.64114012],\n",
       "       [ 0.21038434],\n",
       "       [ 0.32907606],\n",
       "       [ 0.3830465 ],\n",
       "       [ 0.46861597],\n",
       "       [ 0.08514498],\n",
       "       [ 0.        ],\n",
       "       [ 0.43417268],\n",
       "       [ 0.48663468],\n",
       "       [ 0.29453091],\n",
       "       [ 0.42103503],\n",
       "       [ 0.383793  ],\n",
       "       [ 0.25887546],\n",
       "       [ 0.12235214],\n",
       "       [ 0.00473609],\n",
       "       [ 0.47408603],\n",
       "       [ 0.39390696],\n",
       "       [ 0.16037955],\n",
       "       [ 0.12606785],\n",
       "       [ 0.43034571],\n",
       "       [ 0.15181652],\n",
       "       [ 0.19478496],\n",
       "       [ 0.33865823],\n",
       "       [ 0.60947573],\n",
       "       [ 1.        ],\n",
       "       [ 0.37502826],\n",
       "       [ 0.23336995],\n",
       "       [ 0.50671252],\n",
       "       [ 0.39889983],\n",
       "       [ 0.46564269]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_residual_model(is_mnist=is_mnist, img_channels=img_channels, img_rows=img_rows, img_cols=img_cols)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# autosave best Model\n",
    "best_model_file = \"./my_model_weights.h5\"\n",
    "best_model = ModelCheckpoint(best_model_file, verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test, Y_test))#, callbacks=[best_model])\n",
    "score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000L, 1L)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
